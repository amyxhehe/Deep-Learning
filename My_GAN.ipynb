{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP4iZa+VwspnqyB6cDuHLRj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"hjLbTakQc1PG","executionInfo":{"status":"ok","timestamp":1720178391226,"user_tz":-300,"elapsed":625,"user":{"displayName":"Aeman Chaudhary","userId":"07954425247912789483"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"id":"9Pxj_EZX7SWU","executionInfo":{"status":"ok","timestamp":1720178391964,"user_tz":-300,"elapsed":50,"user":{"displayName":"Aeman Chaudhary","userId":"07954425247912789483"}}},"outputs":[],"source":["#the activation functions\n","#used for models where we have to predict the probability as an output\n","#useful for binary classification tasks\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","def sigmoid_derivative(x):\n","    return x * (1 - x)"]},{"cell_type":"code","source":["def initialize_weights(input_dim, hidden_dim, output_dim):\n","    #initializing weights with small random numbers\n","    #initializing biases with zeros\n","    W1 = np.random.randn(input_dim, hidden_dim) * 0.01\n","    b1 = np.zeros((1, hidden_dim))\n","    W2 = np.random.randn(hidden_dim, output_dim) * 0.01\n","    b2 = np.zeros((1, output_dim))\n","    return W1, b1, W2, b2"],"metadata":{"id":"0hba59QOc65C","executionInfo":{"status":"ok","timestamp":1720178391966,"user_tz":-300,"elapsed":48,"user":{"displayName":"Aeman Chaudhary","userId":"07954425247912789483"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["Generator"],"metadata":{"id":"HB77bRS37993"}},{"cell_type":"code","source":["#The generator generates fake data\n","\n","def generator_forward(z, W1, b1, W2, b2):\n","    #the forward pass\n","    #z is a random noise vector\n","    #h are the hidden layer activations\n","    h = sigmoid(np.dot(z, W1) + b1)\n","    generated_data = sigmoid(np.dot(h, W2) + b2)\n","    return generated_data\n","\n","\n","def generator_backward(z, generated_data, d_loss, W1, b1, W2, b2):\n","    #the backward pass\n","    #computes the gradients for the generator's weights and biases using the loss gradient d_loss.\n","    h = sigmoid(np.dot(z, W1) + b1)\n","    d_output = d_loss * sigmoid_derivative(generated_data)\n","\n","    #the gradients\n","    #a gradient measures the change in all weights with regard to the change in error or loss\n","    dW2 = np.dot(h.T, d_output)\n","    db2 = np.sum(d_output, axis=0, keepdims=True)\n","    dh = np.dot(d_output, W2.T) * sigmoid_derivative(h)\n","\n","    dW1 = np.dot(z.T, dh)\n","    db1 = np.sum(dh, axis=0, keepdims=True)\n","\n","    return dW1, db1, dW2, db2"],"metadata":{"id":"zLSsaq6I7xMq","executionInfo":{"status":"ok","timestamp":1720178391968,"user_tz":-300,"elapsed":47,"user":{"displayName":"Aeman Chaudhary","userId":"07954425247912789483"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["Discriminator"],"metadata":{"id":"wFGDMi5r8D7L"}},{"cell_type":"code","source":["#The discriminator classifies fake and real data\n","def discriminator_forward(x, W1, b1, W2, b2):\n","    #forward pass\n","    #x is real or generated data given as input\n","    #h are the hidden layer activations\n","    h = sigmoid(np.dot(x, W1) + b1)\n","    y = sigmoid(np.dot(h, W2) + b2)\n","    return y\n","\n","\n","def discriminator_backward(x, y_true, y_pred, W1, b1, W2, b2):\n","    #backward pass\n","    #calculates the gradients for the discriminator's weights and biases\n","    #uses the true labels y_true and predicted labels y_pred\n","    h = sigmoid(np.dot(x, W1) + b1)\n","    d_output = (y_pred - y_true) * sigmoid_derivative(y_pred)\n","\n","    #the gradients\n","    #a gradient measures the change in all weights with regard to the change in error or loss\n","    dW2 = np.dot(h.T, d_output)\n","    db2 = np.sum(d_output, axis=0, keepdims=True)\n","    dh = np.dot(d_output, W2.T) * sigmoid_derivative(h)\n","\n","    dW1 = np.dot(x.T, dh)\n","    db1 = np.sum(dh, axis=0, keepdims=True)\n","\n","    return dW1, db1, dW2, db2"],"metadata":{"id":"76_i0rlt75b1","executionInfo":{"status":"ok","timestamp":1720178391969,"user_tz":-300,"elapsed":44,"user":{"displayName":"Aeman Chaudhary","userId":"07954425247912789483"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["Training"],"metadata":{"id":"W9seZvs78NBN"}},{"cell_type":"code","source":["def train_gan(real_data, input_dim, hidden_dim, output_dim, latent_dim, epochs=1000, learning_rate=0.01):\n","    #initializing random weights and biases for both the generator and discriminator\n","    g_W1, g_b1, g_W2, g_b2 = initialize_weights(latent_dim, hidden_dim, output_dim)\n","    d_W1, d_b1, d_W2, d_b2 = initialize_weights(output_dim, hidden_dim, 1)\n","\n","    #the training loop\n","    #epoch is the number of times you give the data to the neursl network\n","    #in other words, an epoch means training the neural network with all the training data for one cycle.\n","    for epoch in range(epochs):\n","        for real_sample in real_data:\n","            #training the Discriminator\n","            z = np.random.randn(1, latent_dim)\n","            #generating fake data\n","            generated_data = generator_forward(z, g_W1, g_b1, g_W2, g_b2)\n","\n","            real_sample = real_sample.reshape(1, -1)\n","            #calculating discriminator's output for both real and fake data\n","            d_real = discriminator_forward(real_sample, d_W1, d_b1, d_W2, d_b2)\n","            d_fake = discriminator_forward(generated_data, d_W1, d_b1, d_W2, d_b2)\n","\n","            #calculating discriminator loss for both real and fake data\n","            d_loss_real = -np.log(d_real)\n","            d_loss_fake = -np.log(1 - d_fake)\n","            d_loss = d_loss_real + d_loss_fake\n","\n","            d_loss_gradient_real = (d_real - 1) / (d_real * (1 - d_real))\n","            d_loss_gradient_fake = d_fake / (d_fake * (1 - d_fake))\n","\n","            #calculating discriminator gradients for both real and fake data\n","            dW1_real, db1_real, dW2_real, db2_real = discriminator_backward(real_sample, np.ones((1, 1)), d_real, d_W1, d_b1, d_W2, d_b2)\n","            dW1_fake, db1_fake, dW2_fake, db2_fake = discriminator_backward(generated_data, np.zeros((1, 1)), d_fake, d_W1, d_b1, d_W2, d_b2)\n","\n","            #updating discriminator's weights and biases using gradient descent\n","            #Gradient Descent is an algorithm that is used to optimize the cost function or the error of the model\n","            d_W1 -= learning_rate * (dW1_real + dW1_fake)\n","            d_b1 -= learning_rate * (db1_real + db1_fake)\n","            d_W2 -= learning_rate * (dW2_real + dW2_fake)\n","            d_b2 -= learning_rate * (db2_real + db2_fake)\n","\n","            #training the Generator\n","            z = np.random.randn(1, latent_dim)\n","            #generating fake data using the generator\n","            generated_data = generator_forward(z, g_W1, g_b1, g_W2, g_b2)\n","            #calculating discriminator's output for the fake data\n","            d_fake = discriminator_forward(generated_data, d_W1, d_b1, d_W2, d_b2)\n","\n","            #calculating generator loss or error\n","            g_loss = -np.log(d_fake)\n","            d_loss_gradient = (d_fake - 1) / (d_fake * (1 - d_fake))\n","\n","            ##calculating generator gradients\n","            dW1_gen, db1_gen, dW2_gen, db2_gen = discriminator_backward(generated_data, np.ones((1, 1)), d_fake, d_W1, d_b1, d_W2, d_b2)\n","            dW1_gen, db1_gen, dW2_gen, db2_gen = generator_backward(z, generated_data, d_loss_gradient, g_W1, g_b1, g_W2, g_b2)\n","\n","            #updating generator's weights and biases using gradient descent\n","            #Gradient Descent is an algorithm that is used to optimize the cost function or the error of the model\n","            g_W1 -= learning_rate * dW1_gen\n","            g_b1 -= learning_rate * db1_gen\n","            g_W2 -= learning_rate * dW2_gen\n","            g_b2 -= learning_rate * db2_gen\n","\n","        if epoch % 100 == 0:\n","            print(f'Epoch {epoch}, Discriminator Loss: {d_loss}, Generator Loss: {g_loss}')\n","\n","    return g_W1, g_b1, g_W2, g_b2, d_W1, d_b1, d_W2, d_b2"],"metadata":{"id":"3FnHLD4H8PoB","executionInfo":{"status":"ok","timestamp":1720178391970,"user_tz":-300,"elapsed":43,"user":{"displayName":"Aeman Chaudhary","userId":"07954425247912789483"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["Implementing"],"metadata":{"id":"aZqFa7Pu8VXs"}},{"cell_type":"code","source":["#Example data (randomly generated)\n","real_data = np.random.rand(100, 3)  # 00 samples of real data, each with 3 features\n","\n","#Train the GAN\n","g_W1, g_b1, g_W2, g_b2, d_W1, d_b1, d_W2, d_b2 = train_gan(real_data, input_dim=3, hidden_dim=10, output_dim=3, latent_dim=5, epochs=1000, learning_rate=0.01)"],"metadata":{"id":"KTTYwE9j8YQD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"931632c7-2d0c-4332-ed98-1e99dcbf0e31","executionInfo":{"status":"ok","timestamp":1720178422069,"user_tz":-300,"elapsed":30139,"user":{"displayName":"Aeman Chaudhary","userId":"07954425247912789483"}}},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, Discriminator Loss: [[1.38632109]], Generator Loss: [[0.69051012]]\n","Epoch 100, Discriminator Loss: [[1.38374686]], Generator Loss: [[0.70011644]]\n","Epoch 200, Discriminator Loss: [[1.32341788]], Generator Loss: [[0.99885128]]\n","Epoch 300, Discriminator Loss: [[1.33392437]], Generator Loss: [[1.5170313]]\n","Epoch 400, Discriminator Loss: [[1.33800336]], Generator Loss: [[1.81607187]]\n","Epoch 500, Discriminator Loss: [[1.31943984]], Generator Loss: [[1.99952668]]\n","Epoch 600, Discriminator Loss: [[1.28885323]], Generator Loss: [[2.12773211]]\n","Epoch 700, Discriminator Loss: [[1.25181764]], Generator Loss: [[2.22478225]]\n","Epoch 800, Discriminator Loss: [[1.21103261]], Generator Loss: [[2.30189018]]\n","Epoch 900, Discriminator Loss: [[1.16799473]], Generator Loss: [[2.36520584]]\n"]}]}]}