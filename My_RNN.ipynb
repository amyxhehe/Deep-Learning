{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNYkd4+gbDwyYKkNVx7l5gm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"HJKAVXEPTT-u"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"plAZ7KjxLbSW"},"outputs":[],"source":["#activation function for the output layer\n","#converts raw scores to probabilities\n","def softmax(x):\n","    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n","    return exp_x / exp_x.sum(axis=1, keepdims=True)"]},{"cell_type":"code","source":["#initializes weights randomly\n","def initialize_weights(input_dim, hidden_dim, output_dim):\n","    Wh = np.random.randn(hidden_dim, hidden_dim) * 0.01  #weights used during loops/reccurent computation\n","    Wx = np.random.randn(input_dim, hidden_dim) * 0.01   # weights used from input to hidden layer\n","    Wy = np.random.randn(hidden_dim, output_dim) * 0.01  #weights used from hidden layer to output\n","    bh = np.zeros((1, hidden_dim))  #bias in the hidden layer\n","    by = np.zeros((1, output_dim))  #bias in the output layer\n","    return Wh, Wx, Wy, bh, by"],"metadata":{"id":"hp0Gq63KTatp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Forward pass"],"metadata":{"id":"-kA7yNsDL4xs"}},{"cell_type":"code","source":["def rnn_forward(X, Wh, Wx, Wy, bh, by, h_prev):\n","    #T is the sequence length\n","    T, input_dim = X.shape\n","    hidden_dim, output_dim = Wy.shape\n","\n","    #h holds the hidden states\n","    h = np.zeros((T, hidden_dim))\n","    #y holds the outputs\n","    y = np.zeros((T, output_dim))\n","\n","    #t is the time step\n","    for t in range(T):\n","        h[t] = np.tanh(np.dot(X[t], Wx) + np.dot(h_prev, Wh) + bh) #tanh is the activation function used to calculate the hidden state\n","        y[t] = softmax(np.dot(h[t], Wy) + by) #activation function is used to calculate the output\n","        h_prev = h[t]\n","\n","    return h, y"],"metadata":{"id":"V2EWnqbtLt0O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Backward pass"],"metadata":{"id":"kqekSG1WL_e-"}},{"cell_type":"code","source":["#Computes gradients of the loss with respect to the weights and biases.\n","#A gradient simply measures the change in all weights with regard to the change in error.\n","#The gradient of a function at any point can be calculated as the first-order derivative of that function at that point.\n","def rnn_backward(X, y_true, y_pred, h, Wh, Wx, Wy, bh, by):\n","    T, input_dim = X.shape\n","    hidden_dim, output_dim = Wy.shape\n","\n","    #initializing gradients with zeros\n","    dWh = np.zeros_like(Wh)\n","    dWx = np.zeros_like(Wx)\n","    dWy = np.zeros_like(Wy)\n","    dbh = np.zeros_like(bh)\n","    dby = np.zeros_like(by)\n","\n","    dh_next = np.zeros((1, hidden_dim))\n","\n","    #calculating loss or error using cross-entropy\n","    loss = -np.sum(y_true * np.log(y_pred + 1e-8))\n","\n","    #calculating gradients\n","    #back propagating\n","    for t in reversed(range(T)):\n","        dy = y_pred[t] - y_true[t]\n","        dWy += np.dot(h[t].reshape(-1, 1), dy.reshape(1, -1))\n","        dby += dy\n","\n","        dh = np.dot(dy, Wy.T) + dh_next\n","        dh_raw = (1 - h[t]**2) * dh #derivative of tanh activation function\n","\n","        dbh += dh_raw\n","        dWx += np.dot(X[t].reshape(-1, 1), dh_raw.reshape(1, -1))\n","        dWh += np.dot(h[t-1].reshape(-1, 1), dh_raw.reshape(1, -1)) if t != 0 else np.zeros_like(dWh)\n","\n","        #for next time step\n","        dh_next = np.dot(dh_raw, Wh.T)\n","\n","    return dWh, dWx, dWy, dbh, dby, loss"],"metadata":{"id":"nebNvIrrLx5P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Training loop"],"metadata":{"id":"A0-SvhEGMGy9"}},{"cell_type":"code","source":["def train_rnn(X_train, y_train, input_dim, hidden_dim, output_dim, epochs=1000, learning_rate=0.01):\n","    Wh, Wx, Wy, bh, by = initialize_weights(input_dim, hidden_dim, output_dim)\n","    h_prev = np.zeros((1, hidden_dim))\n","\n","    #the ultimate training loop\n","    #epoch is the number of times you give the data to the neursl network\n","    #in other words, an epoch means training the neural network with all the training data for one cycle.\n","    for epoch in range(epochs):\n","        for X, y_true in zip(X_train, y_train):\n","            h, y_pred = rnn_forward(X, Wh, Wx, Wy, bh, by, h_prev)\n","            dWh, dWx, dWy, dbh, dby, loss = rnn_backward(X, y_true, y_pred, h, Wh, Wx, Wy, bh, by)\n","\n","            #Updating weights and biases using gradient descent.\n","            #Gradient Descent is an algorithm that is used to optimize the cost function or the error of the model.\n","            Wh -= learning_rate * dWh\n","            Wx -= learning_rate * dWx\n","            Wy -= learning_rate * dWy\n","            bh -= learning_rate * dbh\n","            by -= learning_rate * dby\n","\n","        if epoch % 100 == 0:\n","            print(f'Epoch {epoch}, Loss: {loss}')\n","\n","    return Wh, Wx, Wy, bh, by"],"metadata":{"id":"Nwo6oVlBMI03"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Example usage"],"metadata":{"id":"y4gQ2MamMNPj"}},{"cell_type":"code","source":["# Example data (sequences of one-hot encoded vectors)\n","X_train = [np.random.rand(10, 5) for _ in range(100)]  # 100 sequences of length 10, input_dim=5\n","y_train = [np.random.randint(0, 2, (10, 3)) for _ in range(100)]  # 100 sequences of length 10, output_dim=3\n","\n","# Train the RNN\n","Wh, Wx, Wy, bh, by = train_rnn(X_train, y_train, input_dim=5, hidden_dim=10, output_dim=3, epochs=1000, learning_rate=0.01)"],"metadata":{"id":"yLob76hUMPUS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720023735081,"user_tz":-300,"elapsed":65069,"user":{"displayName":"Aeman Chaudhary","userId":"07954425247912789483"}},"outputId":"eff0d68e-2146-4151-f1d1-bb8512ea6765"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, Loss: 16.679187689077008\n","Epoch 100, Loss: 16.376423250610294\n","Epoch 200, Loss: 16.376392185389953\n","Epoch 300, Loss: 16.376400798604063\n","Epoch 400, Loss: 16.376403662318467\n","Epoch 500, Loss: 16.376406048018307\n","Epoch 600, Loss: 16.376408983441152\n","Epoch 700, Loss: 16.376409174312986\n","Epoch 800, Loss: 16.376409191811412\n","Epoch 900, Loss: 16.376408939342323\n"]}]}]}